{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "820bc63c",
   "metadata": {},
   "source": [
    "\n",
    "# SPADE (Spatio-Temporal Pattern Detection) — Step-by-step Notebook\n",
    "\n",
    "This notebook re-creates the SPADE pipeline (binning → transactions → closed frequent patterns → significance via surrogate-based *pattern spectrum* → pattern-set reduction).\n",
    "It assumes your spikes are provided as a NumPy array of shape `(2, n)` where the first row contains spike times (in **milliseconds**) and the second row contains integer neuron IDs in `[0, N-1]`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5defe09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Imports ===\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Tuple, Set, Iterable, Optional\n",
    "import math\n",
    "np.set_printoptions(edgeitems=5, linewidth=120, suppress=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47c96970",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 529)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# (Optional) Synthetic data generator for quick testing.\n",
    "# If you already have `spikes` as a (2, n) array, skip this cell.\n",
    "def make_synthetic_spikes(N=20, duration_ms=5000, rate_hz=5.0, seed=0,\n",
    "                          embed_pattern=True, pattern_size=4, pattern_lag_ms=(0,3,7,12),\n",
    "                          repetitions=8, base_time_ms=500, rep_stride_ms=400):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    # Homogeneous Poisson spikes per neuron\n",
    "    spikes = []\n",
    "    for i in range(N):\n",
    "        # Expected number of spikes:\n",
    "        lam = rate_hz * (duration_ms / 1000.0)\n",
    "        n_i = rng.poisson(lam)\n",
    "        times = rng.uniform(0, duration_ms, size=n_i)\n",
    "        for t in times:\n",
    "            spikes.append((t, i))\n",
    "    # Embed a repeating pattern (same relative lags across a subset of neurons)\n",
    "    if embed_pattern:\n",
    "        neurons = rng.choice(N, size=pattern_size, replace=False)\n",
    "        for r in range(repetitions):\n",
    "            t0 = base_time_ms + r * rep_stride_ms\n",
    "            for j, lag in enumerate(pattern_lag_ms[:pattern_size]):\n",
    "                t = t0 + lag\n",
    "                if 0 <= t < duration_ms:\n",
    "                    spikes.append((t, int(neurons[j])))\n",
    "    spikes = np.array(spikes, dtype=float)\n",
    "    # Sort by time\n",
    "    order = np.argsort(spikes[:,0])\n",
    "    spikes = spikes[order]\n",
    "    # Shape to (2, n): row0 = times_ms, row1 = neuron_ids\n",
    "    spikes_out = np.vstack([spikes[:,0], spikes[:,1].astype(int)]).astype(float)\n",
    "    return spikes_out\n",
    "\n",
    "# Example: comment out if you already have your own `spikes`\n",
    "spikes = make_synthetic_spikes()\n",
    "spikes.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74d500d-bd7b-4007-a83f-0d24d3c64d2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15f33543",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Parameters (edit these) ===\n",
    "# If you already have `spikes` defined as (2, n) [times_ms, neuron_ids], leave it as is.\n",
    "# Otherwise, uncomment the generator above or load your data here.\n",
    "\n",
    "# Required: bin size and window length for pattern mining\n",
    "dt_ms = 1.0                 # raster bin width (ms)\n",
    "window_bins = 20            # K: number of bins in each sliding window\n",
    "min_support = 2             # minimum number of window occurrences for a pattern to be considered frequent\n",
    "max_itemset_size = None        # optional cap on pattern size to keep mining tractable (None for no cap)\n",
    "\n",
    "# Surrogate / significance settings\n",
    "n_surrogates = 20           # number of dither surrogates for pattern-spectrum (keep small for demo; raise for real use)\n",
    "dither_halfwidth_bins = 1   # J: +/- jitter range in bins (e.g., 5 bins = +/- 5 ms if dt_ms=1)\n",
    "alpha = 0.05                # FDR target for PSF; used also as per-test threshold in PSR demo\n",
    "psr_h = 0                   # SPADE paper uses h=0\n",
    "psr_k = 2                   # SPADE paper uses k=2\n",
    "\n",
    "# If you generated synthetic data above, ensure `spikes` exists.\n",
    "try:\n",
    "    spikes  # noqa: F821\n",
    "except NameError:\n",
    "    print(\"Note: `spikes` is not defined. Either run the synthetic generator cell or load your data into a (2, n) array named `spikes`.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e4979e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def bin_spikes(spikes_2xn: np.ndarray, dt_ms: float, t_start_ms: Optional[float]=None, t_end_ms: Optional[float]=None):\n",
    "    \"\"\"\n",
    "    Convert (2, n) spikes [times_ms; neuron_id] to a binary raster of shape (N, B).\n",
    "    - Enforces at most one spike per neuron per bin via clipping to {0,1}.\n",
    "    Returns: raster (bool array), bin_edges_ms (length B+1), neuron_count N\n",
    "    \"\"\"\n",
    "    assert spikes_2xn.shape[0] == 2, \"Input must be (2, n): [times_ms; neuron_ids]\"\n",
    "    times_ms = spikes_2xn[0]\n",
    "    neuron_ids = spikes_2xn[1].astype(int)\n",
    "    N = int(neuron_ids.max()) + 1 if neuron_ids.size > 0 else 0\n",
    "    if t_start_ms is None:\n",
    "        t_start_ms = 0.0 if times_ms.size == 0 else float(np.floor(times_ms.min()))\n",
    "    if t_end_ms is None:\n",
    "        t_end_ms = float(np.ceil(times_ms.max())) if times_ms.size > 0 else 0.0\n",
    "    if t_end_ms < t_start_ms:\n",
    "        t_end_ms = t_start_ms\n",
    "    B = int(np.ceil((t_end_ms - t_start_ms) / dt_ms))\n",
    "    bin_edges_ms = t_start_ms + np.arange(B+1) * dt_ms\n",
    "    raster = np.zeros((N, B), dtype=bool)\n",
    "    if times_ms.size == 0 or N == 0 or B == 0:\n",
    "        return raster, bin_edges_ms, N\n",
    "    # Bin indices\n",
    "    bin_idx = ((times_ms - t_start_ms) / dt_ms).astype(int)\n",
    "    mask = (bin_idx >= 0) & (bin_idx < B) & (neuron_ids >= 0) & (neuron_ids < N)\n",
    "    bin_idx = bin_idx[mask]\n",
    "    neuron_ids = neuron_ids[mask]\n",
    "    # Set raster; clip multi-hits to 1\n",
    "    raster[neuron_ids, bin_idx] = True\n",
    "    return raster, bin_edges_ms, N\n",
    "\n",
    "# Example usage (if you have `spikes`):\n",
    "# raster, bin_edges_ms, N = bin_spikes(spikes, dt_ms)\n",
    "# raster.shape, N\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6112a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_transactions(raster: np.ndarray, window_bins: int) -> Tuple[List[Set[int]], int]:\n",
    "    \"\"\"\n",
    "    Build transactions from raster using a sliding window of `window_bins` bins.\n",
    "    Each transaction is a set of integer-encoded attributes a = i*K + kappa where\n",
    "    i = neuron index, kappa = local time within the window [0..K-1].\n",
    "    Returns:\n",
    "      - transactions: list of sets\n",
    "      - K: the window_bins (redundant, but helpful for decoding)\n",
    "    \"\"\"\n",
    "    N, B = raster.shape\n",
    "    K = int(window_bins)\n",
    "    if K <= 0:\n",
    "        return [], K\n",
    "    transactions: List[Set[int]] = []\n",
    "    # For each window start tau in [0 .. B-K]\n",
    "    for tau in range(0, B - K + 1):\n",
    "        # Collect (i, kappa) where spike present\n",
    "        attrs = set()\n",
    "        # Find indices where any spike in window\n",
    "        # Vectorized: find all (i,b) spikes, then map to kappa=b-tau in [0..K-1]\n",
    "        # But to keep it readable:\n",
    "        window = raster[:, tau:tau+K]\n",
    "        is_i, is_kappa = np.where(window)\n",
    "        for i, kappa in zip(is_i, is_kappa):\n",
    "            attr = int(i * K + kappa)\n",
    "            attrs.add(attr)\n",
    "        transactions.append(attrs)\n",
    "    return transactions, K\n",
    "\n",
    "def decode_attr(attr: int, K: int) -> Tuple[int, int]:\n",
    "    return (attr // K, attr % K)\n",
    "\n",
    "# Example (after binning):\n",
    "# tx, K = make_transactions(raster, window_bins)\n",
    "# len(tx), K, len(tx[0]) if tx else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77149b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass(frozen=True)\n",
    "class Pattern:\n",
    "    items: frozenset  # of int attributes\n",
    "    extent: np.ndarray  # window indices (occurrence windows)\n",
    "    size: int           # z = |items|\n",
    "    support: int        # c = |extent|\n",
    "\n",
    "def _build_inverted_index(transactions: List[Set[int]]) -> Dict[int, np.ndarray]:\n",
    "    \"\"\"Return dict: item -> sorted array of transaction indices where item occurs.\"\"\"\n",
    "    item_to_idxs: Dict[int, List[int]] = {}\n",
    "    for t_idx, trans in enumerate(transactions):\n",
    "        for a in trans:\n",
    "            item_to_idxs.setdefault(a, []).append(t_idx)\n",
    "    return {a: np.array(sorted(v), dtype=int) for a, v in item_to_idxs.items()}\n",
    "\n",
    "def _intersect_extents(extents: Iterable[np.ndarray]) -> np.ndarray:\n",
    "    extents = list(extents)\n",
    "    if not extents:\n",
    "        return np.array([], dtype=int)\n",
    "    # Start with the smallest to intersect efficiently\n",
    "    extents.sort(key=lambda x: x.size)\n",
    "    res = extents[0]\n",
    "    for e in extents[1:]:\n",
    "        res = np.intersect1d(res, e, assume_unique=True)\n",
    "        if res.size == 0:\n",
    "            break\n",
    "    return res\n",
    "\n",
    "def apriori_closed_patterns(transactions: List[Set[int]], min_support: int, max_size: Optional[int]=None) -> List[Pattern]:\n",
    "    \"\"\"\n",
    "    Simple Apriori miner that returns only **closed** frequent itemsets as Pattern objects.\n",
    "    - Uses extent intersections to compute supports efficiently.\n",
    "    - After mining all sizes, filters non-closed sets (those with a strict superset with same support).\n",
    "    \"\"\"\n",
    "    if min_support <= 1 and not transactions:\n",
    "        return []\n",
    "    item_to_idxs = _build_inverted_index(transactions)\n",
    "    # L1 frequent singletons\n",
    "    Lk = []\n",
    "    for a, idxs in item_to_idxs.items():\n",
    "        if idxs.size >= min_support:\n",
    "            Lk.append((frozenset([a]), idxs))\n",
    "    Lk = sorted(Lk, key=lambda x: (len(x[0]), -x[1].size))\n",
    "    all_levels = [Lk]\n",
    "    k = 1\n",
    "    # Generate higher-order candidates\n",
    "    while Lk:\n",
    "        if max_size is not None and k >= max_size:\n",
    "            break\n",
    "        # Join step: unify pairs sharing k-1 items prefix\n",
    "        Ck1 = []\n",
    "        Lk_sorted = sorted(Lk, key=lambda x: tuple(sorted(x[0])))\n",
    "        for i in range(len(Lk_sorted)):\n",
    "            for j in range(i+1, len(Lk_sorted)):\n",
    "                A, extA = Lk_sorted[i]\n",
    "                B, extB = Lk_sorted[j]\n",
    "                A_sorted = tuple(sorted(A))\n",
    "                B_sorted = tuple(sorted(B))\n",
    "                if A_sorted[:k-1] != B_sorted[:k-1]:\n",
    "                    break\n",
    "                candidate = A.union(B)\n",
    "                if len(candidate) != k+1:\n",
    "                    continue\n",
    "                # Prune: all (k)-subsets must be frequent (Apriori)\n",
    "                # (We can skip explicit check since we join only from Lk.)\n",
    "                ext = _intersect_extents([extA, extB])\n",
    "                if ext.size >= min_support:\n",
    "                    Ck1.append((candidate, ext))\n",
    "        # Deduplicate candidates\n",
    "        seen = {}\n",
    "        for items, ext in Ck1:\n",
    "            key = tuple(sorted(items))\n",
    "            if key not in seen:\n",
    "                seen[key] = (items, ext)\n",
    "        Lk = sorted(seen.values(), key=lambda x: (len(x[0]), -x[1].size))\n",
    "        if Lk:\n",
    "            all_levels.append(Lk)\n",
    "        k += 1\n",
    "\n",
    "    # Flatten all frequent itemsets\n",
    "    all_freq = []\n",
    "    for level in all_levels:\n",
    "        for items, ext in level:\n",
    "            all_freq.append((items, ext))\n",
    "\n",
    "    # Filter to closed: remove any itemset that has a strict superset with the same support\n",
    "    # Build support -> list of itemsets\n",
    "    sup_to_sets: Dict[int, List[frozenset]] = {}\n",
    "    for items, ext in all_freq:\n",
    "        sup_to_sets.setdefault(ext.size, []).append(items)\n",
    "\n",
    "    closed = []\n",
    "    for items, ext in all_freq:\n",
    "        is_closed = True\n",
    "        for sup_items in sup_to_sets[ext.size]:\n",
    "            if items < sup_items:  # strict subset\n",
    "                is_closed = False\n",
    "                break\n",
    "        if is_closed:\n",
    "            closed.append(Pattern(frozenset(items), ext, len(items), ext.size))\n",
    "    # Sort by (size desc, support desc)\n",
    "    closed.sort(key=lambda p: (-p.size, -p.support, tuple(sorted(p.items))))\n",
    "    return closed\n",
    "\n",
    "# Example (after transactions):\n",
    "# patterns = apriori_closed_patterns(tx, min_support, max_itemset_size)\n",
    "# len(patterns), patterns[0].size, patterns[0].support if patterns else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f54d808",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def decode_pattern_items(pattern: Pattern, K: int) -> List[Tuple[int, int]]:\n",
    "    return [decode_attr(a, K) for a in sorted(pattern.items)]\n",
    "\n",
    "def pattern_signature(pattern: Pattern) -> Tuple[int, int]:\n",
    "    return (pattern.size, pattern.support)\n",
    "\n",
    "def summarize_patterns(patterns: List[Pattern], K: int, max_show: int=10):\n",
    "    print(f\"Found {len(patterns)} closed frequent patterns.\")\n",
    "    for idx, p in enumerate(patterns[:max_show], 1):\n",
    "        pairs = decode_pattern_items(p, K)\n",
    "        print(f\"{idx:2d}) size z={p.size:2d}, support c={p.support:3d}, items (i,kappa)={pairs}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f010d18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_raster(raster: np.ndarray, dt_ms: float, t_start_ms: float=0.0,\n",
    "                pattern: Optional[Pattern]=None, K: Optional[int]=None, highlight_extent: Optional[np.ndarray]=None):\n",
    "    \"\"\"\n",
    "    Simple raster plot (one figure). Optionally highlights the occurrence windows of a given pattern.\n",
    "    - highlight_extent: array of window start indices (tau indices) where the pattern occurs.\n",
    "    \"\"\"\n",
    "    N, B = raster.shape\n",
    "    fig, ax = plt.subplots(figsize=(10, 4))\n",
    "    # Scatter spikes\n",
    "    is_i, is_b = np.where(raster)\n",
    "    ax.scatter(is_b * dt_ms + t_start_ms, is_i, s=4)\n",
    "    # Highlight occurrence windows (if provided)\n",
    "    if pattern is not None and K is not None and highlight_extent is not None and highlight_extent.size > 0:\n",
    "        for tau in highlight_extent:\n",
    "            x0 = (tau * dt_ms) + t_start_ms\n",
    "            x1 = x0 + K * dt_ms\n",
    "            ax.axvspan(x0, x1, alpha=0.15)\n",
    "    ax.set_xlabel(\"Time (ms)\")\n",
    "    ax.set_ylabel(\"Neuron ID\")\n",
    "    ax.set_title(\"Raster\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b262b1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def dither_raster(raster: np.ndarray, halfwidth_bins: int, rng: Optional[np.random.Generator]=None) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Spike dithering on raster: shift each spike independently by a uniform integer in [-J, +J] bins.\n",
    "    Clips at edges and enforces boolean spikes (collisions are fine -> remain True).\n",
    "    \"\"\"\n",
    "    if rng is None:\n",
    "        rng = np.random.default_rng()\n",
    "    N, B = raster.shape\n",
    "    if halfwidth_bins <= 0:\n",
    "        return raster.copy()\n",
    "    is_i, is_b = np.where(raster)\n",
    "    shifts = rng.integers(-halfwidth_bins, halfwidth_bins+1, size=is_i.size)\n",
    "    new_b = np.clip(is_b + shifts, 0, B-1)\n",
    "    sur = np.zeros_like(raster, dtype=bool)\n",
    "    sur[is_i, new_b] = True\n",
    "    return sur\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "145044ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def pattern_spectrum_pvals(raster: np.ndarray, window_bins: int, min_support: int, max_itemset_size: Optional[int],\n",
    "                           n_surrogates: int, dither_halfwidth_bins: int, rng: Optional[np.random.Generator]=None) -> Dict[Tuple[int,int], float]:\n",
    "    \"\"\"\n",
    "    Compute SPADE-like pattern spectrum:\n",
    "      p(z,c) = fraction of surrogates that produce at least one closed frequent pattern with signature (z,c).\n",
    "    Uses simple Apriori miner above for demo purposes.\n",
    "    Returns dict mapping (z,c) -> p-value (with +1/(K+1) smoothing to avoid zeros).\n",
    "    \"\"\"\n",
    "    if rng is None:\n",
    "        rng = np.random.default_rng()\n",
    "    K = window_bins\n",
    "    B = raster.shape[1]\n",
    "    # Collect signatures across surrogates\n",
    "    sig_hits: Dict[Tuple[int,int], int] = {}\n",
    "    for _ in tqdm(range(n_surrogates), desc=\"Surrogates\"):\n",
    "        sur = dither_raster(raster, dither_halfwidth_bins, rng)\n",
    "        tx, _ = make_transactions(sur, K)\n",
    "        if not tx:\n",
    "            continue\n",
    "        pats = apriori_closed_patterns(tx, min_support, max_itemset_size)\n",
    "        seen = set(pattern_signature(p) for p in pats)\n",
    "        for sig in seen:\n",
    "            sig_hits[sig] = sig_hits.get(sig, 0) + 1\n",
    "    # Convert to p-values with +1 smoothing\n",
    "    pvals = {}\n",
    "    for sig, count in sig_hits.items():\n",
    "        pvals[sig] = (count + 1) / (n_surrogates + 1)\n",
    "    # Any signature not seen in surrogates -> assign p=1 by default (conservative) upon lookup\n",
    "    return pvals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "231c6457",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def benjamini_hochberg(pvals: Dict[Tuple[int,int], float], alpha: float) -> Set[Tuple[int,int]]:\n",
    "    \"\"\"Return the set of signatures passing BH-FDR at level alpha.\"\"\"\n",
    "    if not pvals:\n",
    "        return set()\n",
    "    items = sorted(pvals.items(), key=lambda kv: kv[1])\n",
    "    m = len(items)\n",
    "    passed = set()\n",
    "    p_star = 0.0\n",
    "    k_star = 0\n",
    "    for k, (sig, p) in enumerate(items, start=1):\n",
    "        if p <= alpha * k / m:\n",
    "            p_star = p\n",
    "            k_star = k\n",
    "    for k, (sig, p) in enumerate(items, start=1):\n",
    "        if k <= k_star:\n",
    "            passed.add(sig)\n",
    "    return passed\n",
    "\n",
    "def psf_filter(patterns: List[Pattern], p_spectrum: Dict[Tuple[int,int], float], alpha: float) -> List[Pattern]:\n",
    "    \"\"\"Keep patterns whose signature is significant under BH-FDR based on the surrogate spectrum.\"\"\"\n",
    "    # Build p-values only for signatures present in original patterns\n",
    "    orig_sigs = {pattern_signature(p) for p in patterns}\n",
    "    pvals = {sig: p_spectrum.get(sig, 1.0) for sig in orig_sigs}\n",
    "    sig_keep = benjamini_hochberg(pvals, alpha)\n",
    "    kept = [p for p in patterns if pattern_signature(p) in sig_keep]\n",
    "    return kept\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "150e8d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def psr_reduction(patterns: List[Pattern], p_spectrum: Dict[Tuple[int,int], float], alpha: float,\n",
    "                  h: int=0, k: int=2) -> List[Pattern]:\n",
    "    \"\"\"\n",
    "    Reduce overlapping patterns via SPADE-like conditional tests.\n",
    "    For each pair (A, B) with B ⊂ A:\n",
    "      - test A|B with signature (z_A - z_B + h, c_A)\n",
    "      - test B|A with signature (z_B, c_B - c_A + k)\n",
    "    Keep those that remain significant (p <= alpha). If both fail, keep the one with larger z*c (tie-break on z).\n",
    "    Note: This is a single-pass greedy reduction for demonstration.\n",
    "    \"\"\"\n",
    "    keep = set(range(len(patterns)))\n",
    "    def p_of(sig): return p_spectrum.get(sig, 1.0)\n",
    "    for i in range(len(patterns)):\n",
    "        if i not in keep: \n",
    "            continue\n",
    "        Ai = patterns[i]\n",
    "        for j in range(len(patterns)):\n",
    "            if i == j or j not in keep:\n",
    "                continue\n",
    "            Bj = patterns[j]\n",
    "            # Only compare if Bj is a strict subset of Ai (over items)\n",
    "            if not Bj.items.issubset(Ai.items):\n",
    "                continue\n",
    "            if Bj.items == Ai.items:\n",
    "                continue\n",
    "            zA, cA = Ai.size, Ai.support\n",
    "            zB, cB = Bj.size, Bj.support\n",
    "            sig_A_given_B = (max(1, zA - zB + h), cA)\n",
    "            sig_B_given_A = (zB, max(1, cB - cA + k))\n",
    "            pA = p_of(sig_A_given_B)\n",
    "            pB = p_of(sig_B_given_A)\n",
    "            A_sig = pA <= alpha\n",
    "            B_sig = pB <= alpha\n",
    "            if A_sig and not B_sig:\n",
    "                # Keep A, drop B\n",
    "                keep.discard(j)\n",
    "            elif B_sig and not A_sig:\n",
    "                # Keep B, drop A\n",
    "                keep.discard(i)\n",
    "                break\n",
    "            elif (not A_sig) and (not B_sig):\n",
    "                # Keep the one with larger z*c\n",
    "                scoreA = zA * cA\n",
    "                scoreB = zB * cB\n",
    "                if scoreA > scoreB or (scoreA == scoreB and zA >= zB):\n",
    "                    keep.discard(j)\n",
    "                else:\n",
    "                    keep.discard(i)\n",
    "                    break\n",
    "            # else: both significant -> keep both\n",
    "    return [patterns[idx] for idx in sorted(list(keep))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "738d88a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_spade_pipeline(spikes: np.ndarray, dt_ms: float, window_bins: int,\n",
    "                       min_support: int, max_itemset_size: Optional[int],\n",
    "                       n_surrogates: int, dither_halfwidth_bins: int,\n",
    "                       alpha: float, psr_h: int=0, psr_k: int=2,\n",
    "                       t_start_ms: Optional[float]=None, t_end_ms: Optional[float]=None,\n",
    "                       verbose: bool=True):\n",
    "    # 1) Rasterize\n",
    "    raster, bin_edges_ms, N = bin_spikes(spikes, dt_ms, t_start_ms, t_end_ms)\n",
    "    if verbose:\n",
    "        print(f\"Raster: N={N} neurons, B={raster.shape[1]} bins, dt={dt_ms} ms, window_bins={window_bins}\")\n",
    "    # 2) Transactions\n",
    "    tx, K = make_transactions(raster, window_bins)\n",
    "    if verbose:\n",
    "        print(f\"Transactions: {len(tx)} windows; average items/window ≈ {np.mean([len(t) for t in tx]) if tx else 0:.2f}\")\n",
    "    # 3) Mine closed frequent patterns\n",
    "    patterns = apriori_closed_patterns(tx, min_support, max_itemset_size)\n",
    "    if verbose:\n",
    "        summarize_patterns(patterns, K, max_show=10)\n",
    "    # 4) Pattern spectrum from surrogates\n",
    "    p_spectrum = pattern_spectrum_pvals(raster, window_bins, min_support, max_itemset_size,\n",
    "                                        n_surrogates, dither_halfwidth_bins)\n",
    "    # 5) PSF filtering\n",
    "    psf_kept = psf_filter(patterns, p_spectrum, alpha)\n",
    "    if verbose:\n",
    "        print(f\"PSF-kept patterns: {len(psf_kept)}\")\n",
    "        summarize_patterns(psf_kept, K, max_show=10)\n",
    "    # 6) PSR reduction\n",
    "    final_patterns = psr_reduction(psf_kept, p_spectrum, alpha, h=psr_h, k=psr_k)\n",
    "    if verbose:\n",
    "        print(f\"Final patterns after PSR: {len(final_patterns)}\")\n",
    "        summarize_patterns(final_patterns, K, max_show=10)\n",
    "    return {\n",
    "        \"raster\": raster,\n",
    "        \"bin_edges_ms\": bin_edges_ms,\n",
    "        \"transactions\": tx,\n",
    "        \"K\": K,\n",
    "        \"patterns_all\": patterns,\n",
    "        \"p_spectrum\": p_spectrum,\n",
    "        \"patterns_psf\": psf_kept,\n",
    "        \"patterns_final\": final_patterns,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5afec116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raster: N=20 neurons, B=4997 bins, dt=1.0 ms, window_bins=20\n",
      "Transactions: 4978 windows; average items/window ≈ 2.11\n",
      "Found 991 closed frequent patterns.\n",
      " 1) size z= 4, support c=  8, items (i,kappa)=[(6, 7), (7, 3), (10, 0), (14, 12)]\n",
      " 2) size z= 4, support c=  8, items (i,kappa)=[(6, 8), (7, 4), (10, 1), (14, 13)]\n",
      " 3) size z= 4, support c=  8, items (i,kappa)=[(6, 9), (7, 5), (10, 2), (14, 14)]\n",
      " 4) size z= 4, support c=  8, items (i,kappa)=[(6, 10), (7, 6), (10, 3), (14, 15)]\n",
      " 5) size z= 4, support c=  8, items (i,kappa)=[(6, 11), (7, 7), (10, 4), (14, 16)]\n",
      " 6) size z= 4, support c=  8, items (i,kappa)=[(6, 12), (7, 8), (10, 5), (14, 17)]\n",
      " 7) size z= 4, support c=  8, items (i,kappa)=[(6, 13), (7, 9), (10, 6), (14, 18)]\n",
      " 8) size z= 4, support c=  8, items (i,kappa)=[(6, 14), (7, 10), (10, 7), (14, 19)]\n",
      " 9) size z= 3, support c=  8, items (i,kappa)=[(6, 4), (7, 0), (14, 9)]\n",
      "10) size z= 3, support c=  8, items (i,kappa)=[(6, 5), (7, 1), (14, 10)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Surrogates: 100%|██████████████████████████████████████████████████████████████████████| 20/20 [00:13<00:00,  1.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PSF-kept patterns: 0\n",
      "Found 0 closed frequent patterns.\n",
      "Final patterns after PSR: 0\n",
      "Found 0 closed frequent patterns.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# === Example usage ===\n",
    "# (If you did not generate or load `spikes`, uncomment the generator in Cell 3.)\n",
    "spikes = make_synthetic_spikes()\n",
    "\n",
    "results = run_spade_pipeline(\n",
    "    spikes=spikes,\n",
    "    dt_ms=dt_ms,\n",
    "    window_bins=window_bins,\n",
    "    min_support=min_support,\n",
    "    max_itemset_size=max_itemset_size,\n",
    "    n_surrogates=n_surrogates,\n",
    "    dither_halfwidth_bins=dither_halfwidth_bins,\n",
    "    alpha=alpha,\n",
    "    psr_h=psr_h,\n",
    "    psr_k=psr_k,\n",
    "    verbose=True\n",
    ")\n",
    "raster = results[\"raster\"]\n",
    "K = results[\"K\"]\n",
    "final_patterns = results[\"patterns_final\"]\n",
    "if final_patterns:\n",
    "    # Visualize raster with highlighted windows for the top pattern\n",
    "    plot_raster(raster, dt_ms, pattern=final_patterns[0], K=K, highlight_extent=final_patterns[0].extent)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb60f001",
   "metadata": {},
   "source": [
    "\n",
    "**Notes & caveats**\n",
    "\n",
    "- This notebook uses a simple **Apriori** miner for clarity. SPADE uses efficient closed itemset mining (e.g., FP-growth / Eclat variants). For large data, consider replacing `apriori_closed_patterns` with a faster miner.\n",
    "- The surrogate-based **pattern spectrum** here uses dithering on the raster (`±J` bins). Tune `dither_halfwidth_bins` to your physiological hypothesis (e.g., ±5 ms to disrupt millisecond precision).\n",
    "- The **PSR** step is implemented as a single greedy pass; SPADE describes reciprocal conditional testing. For publication-grade analysis, implement full iterative reduction.\n",
    "- All parameters (`dt_ms`, `window_bins`, `min_support`, `n_surrogates`, `alpha`, `h`, `k`) should be adapted to your dataset and compute budget.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
